# -*- coding: utf-8 -*-
import re
import random
import string
import math
import jieba
from collections import Counter, defaultdict
from config import *
from utils import (extract_emojis, is_emoji, parse_timestamp, clean_text, 
                   calculate_entropy, analyze_single_chars)

jieba.setLogLevel(jieba.logging.INFO)

PUNCTUATION_PATTERN = re.compile(
    r'[\s\.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€""''ï¼ˆï¼‰ã€ã€‘\[\](){}Â·~ï½@#$%^&*\-+=<>/\\|\'\"ã€Šã€‹]'
)

class ChatAnalyzer:
    def __init__(self, data):
        self.data = data
        self.messages = data.get('messages', [])
        self.chat_name = data.get('chatName', data.get('chatInfo', {}).get('name', 'æœªçŸ¥ç¾¤èŠ'))
        self.uin_to_name = {}
        self.msgid_to_sender = {}
        self.word_freq = Counter()
        self.word_samples = defaultdict(list)
        self.word_contributors = defaultdict(Counter)
        self.user_msg_count = Counter()
        self.user_char_count = Counter()
        self.user_char_per_msg = {}
        self.user_image_count = Counter()
        self.user_forward_count = Counter()
        self.user_reply_count = Counter()
        self.user_replied_count = Counter()
        self.user_at_count = Counter()
        self.user_ated_count = Counter()
        self.user_emoji_count = Counter()
        self.user_link_count = Counter()
        self.user_night_count = Counter()
        self.user_morning_count = Counter()
        self.user_repeat_count = Counter()
        self.hour_distribution = Counter()
        self.discovered_words = set()
        self.merged_words = {}
        self.single_char_stats = {}  # å•å­—ç»Ÿè®¡
        self.cleaned_texts = []  # ç¼“å­˜æ¸…æ´—åçš„æ–‡æœ¬
        self._build_mappings()

    def _build_mappings(self):
        for msg in self.messages:
            sender = msg.get('sender', {})
            uin = sender.get('uin')
            name = sender.get('name')
            msg_id = msg.get('messageId')
            if uin and name:
                self.uin_to_name[uin] = name
            if msg_id and uin:
                self.msgid_to_sender[msg_id] = uin

    def get_name(self, uin):
        return self.uin_to_name.get(uin, f"æœªçŸ¥ç”¨æˆ·({uin})")

    def analyze(self):
        print(f"ğŸ“Š å¼€å§‹åˆ†æ: {self.chat_name}")
        print(f"ğŸ“ æ¶ˆæ¯æ•°: {len(self.messages)}")
        print("=" * CONSOLE_WIDTH)
        
        print("\nğŸ§¹ é¢„å¤„ç†æ–‡æœ¬...")
        self._preprocess_texts()
        
        print("ğŸ”¤ åˆ†æå•å­—ç‹¬ç«‹æ€§...")
        self.single_char_stats = analyze_single_chars(self.cleaned_texts)
        
        print("ğŸ” æ–°è¯å‘ç°...")
        self._discover_new_words()
        
        print("ğŸ”— è¯ç»„åˆå¹¶...")
        self._merge_word_pairs()
        
        print("ğŸ“ˆ åˆ†è¯ç»Ÿè®¡...")
        self._tokenize_and_count()
        
        print("ğŸ® è¶£å‘³ç»Ÿè®¡...")
        self._fun_statistics()
        
        print("ğŸ§¹ è¿‡æ»¤æ•´ç†...")
        self._filter_results()
        
        print("\nâœ… å®Œæˆ!")

    def _preprocess_texts(self):
        """é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬"""
        skipped = 0
        for msg in self.messages:
            content = msg.get('content', {})
            text = content.get('text', '') if isinstance(content, dict) else ''
            cleaned = clean_text(text)
            if cleaned and len(cleaned) >= 1:
                self.cleaned_texts.append(cleaned)
            elif text:
                skipped += 1
        print(f"   æœ‰æ•ˆæ–‡æœ¬: {len(self.cleaned_texts)} æ¡, è·³è¿‡: {skipped} æ¡")

    def _discover_new_words(self):
        """æ–°è¯å‘ç°"""
        ngram_freq = Counter()
        left_neighbors = defaultdict(Counter)
        right_neighbors = defaultdict(Counter)
        total_chars = 0
        
        for text in self.cleaned_texts:
            # æŒ‰æ ‡ç‚¹åˆ†å¥
            sentences = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰\s\n\r,\.!?\(\)]', text)
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 2:
                    continue
                total_chars += len(sentence)
                
                for n in range(2, min(6, len(sentence) + 1)):
                    for i in range(len(sentence) - n + 1):
                        ngram = sentence[i:i+n]
                        # è·³è¿‡çº¯æ•°å­—/ç¬¦å·/çº¯è‹±æ–‡
                        if re.match(r'^[\d\s\W]+$', ngram) or re.match(r'^[a-zA-Z]+$', ngram):
                            continue
                        ngram_freq[ngram] += 1
                        if i > 0:
                            left_neighbors[ngram][sentence[i-1]] += 1
                        else:
                            left_neighbors[ngram]['<BOS>'] += 1
                        if i + n < len(sentence):
                            right_neighbors[ngram][sentence[i+n]] += 1
                        else:
                            right_neighbors[ngram]['<EOS>'] += 1
        
        # ç­›é€‰æ–°è¯
        for word, freq in ngram_freq.items():
            if freq < NEW_WORD_MIN_FREQ:
                continue
            
            # é‚»æ¥ç†µ
            left_ent = calculate_entropy(left_neighbors[word])
            right_ent = calculate_entropy(right_neighbors[word])
            min_ent = min(left_ent, right_ent)
            if min_ent < ENTROPY_THRESHOLD:
                continue
            
            # PMIï¼ˆå†…éƒ¨å‡èšåº¦ï¼‰
            min_pmi = float('inf')
            for i in range(1, len(word)):
                left_freq = ngram_freq.get(word[:i], 0)
                right_freq = ngram_freq.get(word[i:], 0)
                if left_freq > 0 and right_freq > 0:
                    pmi = math.log2((freq * total_chars) / (left_freq * right_freq + 1e-10))
                    min_pmi = min(min_pmi, pmi)
            
            if min_pmi == float('inf'):
                min_pmi = 0
            
            if min_pmi < PMI_THRESHOLD:
                continue
            
            self.discovered_words.add(word)
        
        # æ·»åŠ åˆ°jiebaè¯å…¸
        for word in self.discovered_words:
            jieba.add_word(word, freq=1000)
        
        print(f"   å‘ç° {len(self.discovered_words)} ä¸ªæ–°è¯")

    def _merge_word_pairs(self):
        """è¯ç»„åˆå¹¶"""
        bigram_counter = Counter()
        word_right_counter = Counter()
        
        for text in self.cleaned_texts:
            words = [w for w in jieba.cut(text) if w.strip()]
            for i in range(len(words) - 1):
                w1, w2 = words[i].strip(), words[i+1].strip()
                if not w1 or not w2:
                    continue
                if re.match(r'^[\d\W]+$', w1) or re.match(r'^[\d\W]+$', w2):
                    continue
                bigram_counter[(w1, w2)] += 1
                word_right_counter[w1] += 1
        
        # æ‰¾å‡ºåº”è¯¥åˆå¹¶çš„è¯å¯¹
        for (w1, w2), count in bigram_counter.items():
            merged = w1 + w2
            if len(merged) > MERGE_MAX_LEN:
                continue
            if count < MERGE_MIN_FREQ:
                continue
            
            # æ¡ä»¶æ¦‚ç‡ P(w2|w1)
            if word_right_counter[w1] > 0:
                prob = count / word_right_counter[w1]
                if prob >= MERGE_MIN_PROB:
                    self.merged_words[merged] = (w1, w2, count, prob)
                    jieba.add_word(merged, freq=count * 1000)
        
        print(f"   åˆå¹¶ {len(self.merged_words)} ä¸ªè¯ç»„")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ª
        if self.merged_words:
            sorted_merges = sorted(self.merged_words.items(), key=lambda x: -x[1][2])[:10]
            for merged, (w1, w2, cnt, prob) in sorted_merges:
                print(f"      {merged}: {w1}+{w2} ({cnt}æ¬¡, {prob:.0%})")

    def _tokenize_and_count(self):
        """åˆ†è¯ç»Ÿè®¡"""
        for idx, msg in enumerate(self.messages):
            sender_uin = msg.get('sender', {}).get('uin')
            content = msg.get('content', {})
            text = content.get('text', '') if isinstance(content, dict) else ''
            original_text = text
            cleaned = clean_text(text)
            
            if not cleaned:
                continue
            
            words = list(jieba.cut(cleaned))
            emojis = extract_emojis(cleaned)
            words = [w for w in words if not is_emoji(w)]  # æ–°å¢ï¼šä»wordsä¸­å»æ‰emoji
            all_tokens = words + emojis
            
            for word in all_tokens:
                word = word.strip()
                if not word:
                    continue
                
                # è·³è¿‡çº¯æ•°å­—/ç¬¦å·
                if re.match(r'^[\d\W]+$', word) and not is_emoji(word):
                    continue
                
                self.word_freq[word] += 1
                if sender_uin:
                    self.word_contributors[word][sender_uin] += 1
                if len(self.word_samples[word]) < SAMPLE_COUNT * 3:
                    self.word_samples[word].append(cleaned)

    def _fun_statistics(self):
        """è¶£å‘³ç»Ÿè®¡"""
        prev_clean = None  # æ”¹ç”¨æ¸…ç†åæ–‡æœ¬
        prev_sender = None
        
        for msg in self.messages:
            sender_uin = msg.get('sender', {}).get('uin')
            if not sender_uin:
                continue
            
            content = msg.get('content', {})
            text = content.get('text', '') if isinstance(content, dict) else ''
            timestamp = msg.get('timestamp', '')
            
            self.user_msg_count[sender_uin] += 1
            clean = clean_text(text)
            self.user_char_count[sender_uin] += len(clean)
            
            # å›¾ç‰‡æ£€æµ‹ï¼ˆæ’é™¤gifï¼‰
            if '[å›¾ç‰‡:' in text:
                if '.gif' not in text.lower():
                    self.user_image_count[sender_uin] += 1
            
            # è½¬å‘æ£€æµ‹
            if '[åˆå¹¶è½¬å‘:' in text:
                self.user_forward_count[sender_uin] += 1
            
            # å›å¤ç»Ÿè®¡
            reply_info = content.get('reply') if isinstance(content, dict) else None
            if reply_info:
                self.user_reply_count[sender_uin] += 1
                ref_msg_id = reply_info.get('referencedMessageId')
                if ref_msg_id and ref_msg_id in self.msgid_to_sender:
                    target_uin = self.msgid_to_sender[ref_msg_id]
                    self.user_replied_count[target_uin] += 1
            
            # @ç»Ÿè®¡
            raw = msg.get('rawMessage', {})
            elements = raw.get('elements', [])
            for elem in elements:
                if elem.get('elementType') == 1:
                    text_elem = elem.get('textElement', {})
                    at_type = text_elem.get('atType', 0)
                    at_uid = text_elem.get('atUid', '')
                    if at_type > 0 and at_uid and at_uid != '0':
                        self.user_at_count[sender_uin] += 1
                        self.user_ated_count[at_uid] += 1
            
            # è¡¨æƒ…ç»Ÿè®¡ï¼ˆåŒ…æ‹¬emojiã€[è¡¨æƒ…:]ã€gifï¼‰
            emojis = extract_emojis(clean)
            gif_count = text.lower().count('.gif')
            bracket_emoji_count = text.count('[è¡¨æƒ…:')
            emoji_count = len(emojis) + bracket_emoji_count + gif_count
            if emoji_count > 0:
                self.user_emoji_count[sender_uin] += emoji_count
            
            # é“¾æ¥ç»Ÿè®¡
            if '[é“¾æ¥:' in text or re.search(r'https?://', text):
                self.user_link_count[sender_uin] += 1
            
            # æ—¶æ®µç»Ÿè®¡
            hour = parse_timestamp(timestamp)
            if hour is not None:
                self.hour_distribution[hour] += 1
                if hour in NIGHT_OWL_HOURS:
                    self.user_night_count[sender_uin] += 1
                if hour in EARLY_BIRD_HOURS:
                    self.user_morning_count[sender_uin] += 1
            
            # å¤è¯»ç»Ÿè®¡ï¼ˆç”¨æ¸…ç†åæ–‡æœ¬ï¼Œä¸”å†…å®¹è¦æœ‰æ„ä¹‰ï¼‰
            if clean and len(clean) >= 2:
                if clean == prev_clean and sender_uin != prev_sender:
                    self.user_repeat_count[sender_uin] += 1
            
            prev_clean = clean if clean else prev_clean  # ç©ºæ¶ˆæ¯ä¸æ›´æ–°
            prev_sender = sender_uin
        
        # è®¡ç®—äººå‡å­—æ•°
        for uin in self.user_msg_count:
            msg_count = self.user_msg_count[uin]
            char_count = self.user_char_count[uin]
            if msg_count >= 10:
                self.user_char_per_msg[uin] = char_count / msg_count

    def _filter_results(self):
        """è¿‡æ»¤ç»“æœ"""
        filtered_freq = Counter()
        
        for word, freq in self.word_freq.items():
            # é•¿åº¦è¿‡æ»¤
            if len(word) < MIN_WORD_LEN or len(word) > MAX_WORD_LEN:
                continue
            if freq < MIN_FREQ:
                continue
            
            # ç™½åå•ç›´æ¥é€šè¿‡
            if word in WHITELIST:
                filtered_freq[word] = freq
                continue
            
            # é»‘åå•è·³è¿‡
            if word in BLACKLIST:
                continue
            
            # åœç”¨è¯ï¼ˆemojié™¤å¤–ï¼‰
            if word in STOPWORDS and not is_emoji(word):
                continue
            
            # å•å­—ç‰¹æ®Šå¤„ç†ï¼ˆé‡‡ç”¨æ—§ç‰ˆé€»è¾‘ï¼‰
            if len(word) == 1:
                if is_emoji(word):
                    pass  # emojiä¿ç•™
                else:
                    stats = self.single_char_stats.get(word)
                    if stats:
                        total, indep, ratio = stats
                        if ratio < SINGLE_MIN_SOLO_RATIO or indep < SINGLE_MIN_SOLO_COUNT:
                            continue
                    else:
                        continue
            
            # çº¯æ•°å­—è·³è¿‡
            if re.match(r'^[\d\s]+$', word):
                continue
            
            # çº¯æ ‡ç‚¹è·³è¿‡
            if all(c in string.punctuation or c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€""''ï¼ˆï¼‰ã€ã€‘' for c in word):
                continue
            
            filtered_freq[word] = freq
        
        self.word_freq = filtered_freq
        
        # é‡‡æ ·
        for word in self.word_samples:
            samples = self.word_samples[word]
            if len(samples) > SAMPLE_COUNT:
                self.word_samples[word] = random.sample(samples, SAMPLE_COUNT)
        
        print(f"   è¿‡æ»¤å {len(self.word_freq)} ä¸ªè¯")

    def get_top_words(self, n=None):
        n = n or TOP_N
        return self.word_freq.most_common(n)

    def get_word_detail(self, word):
        return {
            'word': word,
            'freq': self.word_freq.get(word, 0),
            'samples': self.word_samples.get(word, []),
            'contributors': [(self.get_name(uin), count) 
                           for uin, count in self.word_contributors[word].most_common(CONTRIBUTOR_TOP_N)]
        }

    def get_fun_rankings(self):
        rankings = {}
        
        def fmt(counter, top_n=RANK_TOP_N):
            return [(self.get_name(uin), count) for uin, count in counter.most_common(top_n)]
        
        rankings['è¯ç—¨æ¦œ'] = fmt(self.user_msg_count)
        rankings['å­—æ•°æ¦œ'] = fmt(self.user_char_count)
        
        sorted_avg = sorted(self.user_char_per_msg.items(), key=lambda x: x[1], reverse=True)[:RANK_TOP_N]
        rankings['é•¿æ–‡ç‹'] = [(self.get_name(uin), f"{avg:.1f}å­—/æ¡") for uin, avg in sorted_avg]
        
        rankings['å›¾ç‰‡ç‹‚é­”'] = fmt(self.user_image_count)
        rankings['åˆå¹¶è½¬å‘ç‹'] = fmt(self.user_forward_count)
        rankings['å›å¤ç‹‚'] = fmt(self.user_reply_count)
        rankings['è¢«å›å¤æœ€å¤š'] = fmt(self.user_replied_count)
        rankings['è‰¾ç‰¹ç‹‚'] = fmt(self.user_at_count)
        rankings['è¢«è‰¾ç‰¹æœ€å¤š'] = fmt(self.user_ated_count)
        rankings['è¡¨æƒ…å¸'] = fmt(self.user_emoji_count)
        rankings['é“¾æ¥åˆ†äº«ç‹'] = fmt(self.user_link_count)
        rankings['æ·±å¤œå…š'] = fmt(self.user_night_count)
        rankings['æ—©èµ·é¸Ÿ'] = fmt(self.user_morning_count)
        rankings['å¤è¯»æœº'] = fmt(self.user_repeat_count)
        
        return rankings
    
    def export_json(self):
        """å¯¼å‡ºJSONæ ¼å¼ç»“æœï¼ˆåŒ…å«uinä¿¡æ¯ï¼‰"""
        result = {
            'chatName': self.chat_name,
            'messageCount': len(self.messages),
            'topWords': [
                {
                    'word': word,
                    'freq': freq,
                    'contributors': [
                        {
                            'name': self.get_name(uin), 
                            'uin': uin,
                            'count': count
                        }
                        for uin, count in self.word_contributors[word].most_common(CONTRIBUTOR_TOP_N)
                    ],
                    'samples': self.word_samples.get(word, [])[:SAMPLE_COUNT]
                }
                for word, freq in self.get_top_words()
            ],
            'rankings': {},
            'hourDistribution': {str(h): self.hour_distribution.get(h, 0) for h in range(24)}
        }
        
        # è¶£å‘³æ¦œå•ï¼ˆåŒ…å«uinï¼‰
        def fmt_with_uin(counter, top_n=RANK_TOP_N):
            return [
                {'name': self.get_name(uin), 'uin': uin, 'value': count}
                for uin, count in counter.most_common(top_n)
            ]
        
        result['rankings']['è¯ç—¨æ¦œ'] = fmt_with_uin(self.user_msg_count)
        result['rankings']['å­—æ•°æ¦œ'] = fmt_with_uin(self.user_char_count)
        
        # é•¿æ–‡ç‹ç‰¹æ®Šå¤„ç†
        sorted_avg = sorted(self.user_char_per_msg.items(), key=lambda x: x[1], reverse=True)[:RANK_TOP_N]
        result['rankings']['é•¿æ–‡ç‹'] = [
            {'name': self.get_name(uin), 'uin': uin, 'value': f"{avg:.1f}å­—/æ¡"}
            for uin, avg in sorted_avg
        ]
        
        result['rankings']['å›¾ç‰‡ç‹‚é­”'] = fmt_with_uin(self.user_image_count)
        result['rankings']['åˆå¹¶è½¬å‘ç‹'] = fmt_with_uin(self.user_forward_count)
        result['rankings']['å›å¤ç‹‚'] = fmt_with_uin(self.user_reply_count)
        result['rankings']['è¢«å›å¤æœ€å¤š'] = fmt_with_uin(self.user_replied_count)
        result['rankings']['è‰¾ç‰¹ç‹‚'] = fmt_with_uin(self.user_at_count)
        result['rankings']['è¢«è‰¾ç‰¹æœ€å¤š'] = fmt_with_uin(self.user_ated_count)
        result['rankings']['è¡¨æƒ…å¸'] = fmt_with_uin(self.user_emoji_count)
        result['rankings']['é“¾æ¥åˆ†äº«ç‹'] = fmt_with_uin(self.user_link_count)
        result['rankings']['æ·±å¤œå…š'] = fmt_with_uin(self.user_night_count)
        result['rankings']['æ—©èµ·é¸Ÿ'] = fmt_with_uin(self.user_morning_count)
        result['rankings']['å¤è¯»æœº'] = fmt_with_uin(self.user_repeat_count)
        
        return result
